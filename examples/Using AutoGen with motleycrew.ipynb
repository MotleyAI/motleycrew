{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a864294",
   "metadata": {},
   "source": [
    "# Using AutoGen with motleycrew\n",
    "\n",
    "Microsoft AutoGen is one of the most popular multi-agent frameworks. Motleycrew supports interaction with autogen in both directions: firstly, you can wrap an AutoGen chat as a motleycrew tool, which you can then give to any supported agent; secondly, you can give any motleycrew tool (which includes agents used as tools) to an AutoGen agent. \n",
    "\n",
    "The two options are described in detail below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4360c4e9-db54-4ecd-a2c1-0c651e678578",
   "metadata": {},
   "source": [
    "Let's install external dependencies for the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a800a778-92fc-4a37-835c-37ff07f787a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: duckduckgo-search in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (8.1.1)\n",
      "Requirement already satisfied: click>=8.1.8 in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from duckduckgo-search) (8.2.1)\n",
      "Requirement already satisfied: primp>=0.15.0 in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from duckduckgo-search) (0.15.0)\n",
      "Requirement already satisfied: lxml>=5.3.0 in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from duckduckgo-search) (6.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ag2 in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (0.9.7)\n",
      "Requirement already satisfied: anyio<5.0.0,>=3.0.0 in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from ag2) (4.9.0)\n",
      "Requirement already satisfied: asyncer==0.0.8 in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from ag2) (0.0.8)\n",
      "Requirement already satisfied: diskcache in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from ag2) (5.6.3)\n",
      "Requirement already satisfied: docker in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from ag2) (7.1.0)\n",
      "Requirement already satisfied: httpx<1,>=0.28.1 in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from ag2) (0.28.1)\n",
      "Requirement already satisfied: packaging in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from ag2) (25.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6.1 in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from ag2) (2.11.7)\n",
      "Requirement already satisfied: python-dotenv in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from ag2) (1.1.1)\n",
      "Requirement already satisfied: termcolor in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from ag2) (3.1.0)\n",
      "Requirement already satisfied: tiktoken in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from ag2) (0.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from anyio<5.0.0,>=3.0.0->ag2) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from anyio<5.0.0,>=3.0.0->ag2) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from anyio<5.0.0,>=3.0.0->ag2) (4.14.1)\n",
      "Requirement already satisfied: certifi in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from httpx<1,>=0.28.1->ag2) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from httpx<1,>=0.28.1->ag2) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.28.1->ag2) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from pydantic<3,>=2.6.1->ag2) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from pydantic<3,>=2.6.1->ag2) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from pydantic<3,>=2.6.1->ag2) (0.4.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from docker->ag2) (2.32.4)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from docker->ag2) (2.5.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from tiktoken->ag2) (2025.7.34)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/whimo/motleycrew/.venv/lib/python3.11/site-packages (from requests>=2.26.0->docker->ag2) (3.4.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install duckduckgo-search\n",
    "%pip install ag2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6602c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888adcb2",
   "metadata": {},
   "source": [
    "## Integrating an AutoGen chat into motleycrew\n",
    "\n",
    "If you want to use an AutoGen chat (say one from their many wonderful examples, or one you have already) as part of a motleycrew setup, you can wrap it as a tool, which you can then give to any other motleycrew-supported agent.\n",
    "\n",
    "Let's create an AutoGen chat for automatic problem solving.\n",
    "The code is taken from the example here: https://microsoft.github.io/autogen/docs/notebooks/agentchat_groupchat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b3da0bc-d0f6-4f9c-8e69-0ad126f3a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "import os\n",
    "\n",
    "llm_config = {\n",
    "    \"config_list\": [{\"model\": \"gpt-4.1\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}],\n",
    "    \"cache_seed\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3f7738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    system_message=\"A human admin.\",\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 2,\n",
    "        \"work_dir\": \"examples/data/groupchat\",\n",
    "        \"use_docker\": False,\n",
    "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    "    max_consecutive_auto_reply=1,  # Limit auto-replies to prevent infinite loops\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "coder = autogen.AssistantAgent(\n",
    "    name=\"Coder\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "pm = autogen.AssistantAgent(\n",
    "    name=\"Product_manager\",\n",
    "    system_message=\"Creative in software product ideas.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, coder, pm], messages=[], max_round=12)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8480361",
   "metadata": {},
   "source": [
    "If we were using plain AutoGen, we'd start the chat with something like this:\n",
    "```\n",
    "user_proxy.initiate_chat(\n",
    "    manager, message=\"Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\"\n",
    ")\n",
    "```\n",
    "\n",
    "You see, the chat accepts an input and returns an output, just like a tool.  \n",
    "Because of that, for using the chat in motleycrew, we can utilize the built-in AutoGenChatTool. Its prompt can either be a template or a plain string. Here we are creating a tool that searches arXiv for recent papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d610dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/whimo/motleycrew/.venv/lib/python3.11/site-packages/lunary/__init__.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    }
   ],
   "source": [
    "from motleycrew.tools.autogen_chat_tool import AutoGenChatTool\n",
    "\n",
    "knowledge_retrieval_tool = AutoGenChatTool(\n",
    "    name=\"retrieve_knowledge_by_topic\",\n",
    "    description=\"Search arxiv for the latest paper on a given topic \"\n",
    "                \"and find its potential applications in software.\",  # will be used in the prompt of the future agent that will use the tool\n",
    "    prompt=\"Find a latest paper about {topic} on arxiv \"\n",
    "            \"and find its potential applications in software.\",  # this is the initial prompt for the AutoGen chat itself\n",
    "    initiator=user_proxy,\n",
    "    recipient=manager,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c580e262",
   "metadata": {},
   "source": [
    "We can now give the tool to any agent and solve tasks with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "534453a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from motleycrew import MotleyCrew\n",
    "from motleycrew.agents.langchain import ReActToolCallingMotleyAgent\n",
    "\n",
    "crew = MotleyCrew()\n",
    "writer = ReActToolCallingMotleyAgent(tools=[knowledge_retrieval_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59d9d90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from motleycrew.tasks import SimpleTask\n",
    "\n",
    "blog_post_task = SimpleTask(\n",
    "        crew = crew,\n",
    "        name=\"Produce blog post on the applications of latest advancements related to GPT-4\",\n",
    "        description=\"Using the insights provided by searching research papers, develop an engaging blog \"\n",
    "                    \"post that highlights the most significant advancements on GPT-4 ant their applications.\\n\"\n",
    "                    \"Your post should be informative yet accessible, catering to a tech-savvy audience.\\n\"\n",
    "                    \"Make it sound cool, avoid complex words so it doesn't sound like AI. \"\n",
    "                    \"Create a blog post of at least 4 paragraphs.\",\n",
    "        agent=writer,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf0c1a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Find a latest paper about GPT-4 advancements and applications on arxiv and find its potential applications in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Coder\n",
      "\u001b[0m\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "Plan:\n",
      "1. Use Python to search arXiv for the latest paper about \"GPT-4 advancements and applications\".\n",
      "2. Display the paper title, abstract, and arXiv link.\n",
      "3. Use the abstract to analyze and suggest potential software applications myself.\n",
      "\n",
      "First, let's execute the Python code to fetch and present info about the latest relevant paper from arXiv.\n",
      "\n",
      "```python\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "\n",
      "# Search query parameters\n",
      "search_query = \"GPT-4 advancements applications\"\n",
      "url = (f\"http://export.arxiv.org/api/query?search_query=all:{search_query}\"\n",
      "       \"&start=0&max_results=1&sortBy=submittedDate&sortOrder=descending\")\n",
      "\n",
      "response = requests.get(url)\n",
      "root = ET.fromstring(response.content)\n",
      "\n",
      "# Extract title, summary, and arxiv link\n",
      "ns = {'atom': 'http://www.w3.org/2005/Atom'}\n",
      "entry = root.find('atom:entry', ns)\n",
      "\n",
      "if entry is not None:\n",
      "    title = entry.find('atom:title', ns).text.strip()\n",
      "    summary = entry.find('atom:summary', ns).text.strip()\n",
      "    links = entry.findall('atom:link', ns)\n",
      "    pdf_link = ''\n",
      "    for link in links:\n",
      "        if link.attrib.get('title') == 'pdf':\n",
      "            pdf_link = link.attrib['href']\n",
      "            break\n",
      "    arxiv_id = entry.find('atom:id', ns).text.strip()\n",
      "    print(\"Title:\", title)\n",
      "    print(\"\\nAbstract:\", summary)\n",
      "    print(\"\\nPDF Link:\", pdf_link if pdf_link else arxiv_id)\n",
      "else:\n",
      "    print(\"No recent GPT-4 papers found with the searched keywords.\")\n",
      "```\n",
      "Please execute the above code and share the output so I can analyze the abstract and propose specific potential software applications based on the findings.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: User_proxy\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Title: SUB: Benchmarking CBM Generalization via Synthetic Attribute\n",
      "  Substitutions\n",
      "\n",
      "Abstract: Concept Bottleneck Models (CBMs) and other concept-based interpretable models\n",
      "show great promise for making AI applications more transparent, which is\n",
      "essential in fields like medicine. Despite their success, we demonstrate that\n",
      "CBMs struggle to reliably identify the correct concepts under distribution\n",
      "shifts. To assess the robustness of CBMs to concept variations, we introduce\n",
      "SUB: a fine-grained image and concept benchmark containing 38,400 synthetic\n",
      "images based on the CUB dataset. To create SUB, we select a CUB subset of 33\n",
      "bird classes and 45 concepts to generate images which substitute a specific\n",
      "concept, such as wing color or belly pattern. We introduce a novel Tied\n",
      "Diffusion Guidance (TDG) method to precisely control generated images, where\n",
      "noise sharing for two parallel denoising processes ensures that both the\n",
      "correct bird class and the correct attribute are generated. This novel\n",
      "benchmark enables rigorous evaluation of CBMs and similar interpretable models,\n",
      "contributing to the development of more robust methods. Our code is available\n",
      "at https://github.com/ExplainableML/sub and the dataset at\n",
      "http://huggingface.co/datasets/Jessica-bader/SUB.\n",
      "\n",
      "PDF Link: http://arxiv.org/pdf/2507.23784v1\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Coder\n",
      "\u001b[0m\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "Analysis:\n",
      "\n",
      "The most recent paper fitting your search is titled:\n",
      "\n",
      "**\"SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions\"**\n",
      "\n",
      "This paper, while not focused directly on GPT-4, centers around interpretable machine learning (Concept Bottleneck Models, CBMs) and their benchmarking under distribution shifts, which is related to advancing the transparency and reliability of AI systems like large language models (e.g., GPT-4).\n",
      "\n",
      "### Paper Summary (from abstract)\n",
      "- **Goal**: Make AI applications (like in medicine) more transparent and interpretable.\n",
      "- **Contribution**: Introduces the SUB benchmark with synthetic images and concept substitutions to rigorously test the robustness/generalization of interpretable models.\n",
      "- **Technical novelty**: Proposes a new generation method (TDG) and provides an open benchmark for robust evaluation.\n",
      "\n",
      "### Potential Applications in Software\n",
      "\n",
      "Based on this research, here are possible applications in software:\n",
      "1. **Interpretable AI Systems in Medicine**\n",
      "   - Integrate robust CBMs for medical imaging diagnostic software to provide clinicians not only with predictions but also with clear, visually-explained reasons (attributes) for those predictions—even under shifting conditions (e.g., new imaging hardware, populations).\n",
      "2. **Model Robustness Testing Toolchains**\n",
      "   - Build software tools for ML engineers (applicable to GPT-4-sized models too) to test how robust their model interpretations are when key attributes are synthetically altered.\n",
      "3. **Generative Benchmark Datasets**\n",
      "   - Use the provided synthetic data generation techniques (such as TDG) to automatically create new benchmarks for evaluating LLM interpretability and robustness across domains (text, images, multi-modal).\n",
      "4. **Explainability Plugins for LLMs**\n",
      "   - Implement software modules that can translate LLM hidden states into user-friendly concept attributions, evaluated by such synthetic benchmarks to check if explanations stay robust under modifications.\n",
      "5. **Educational Software**\n",
      "   - Develop platforms for teaching ML interpretability, using the SUB dataset and benchmarks, to train students and practitioners to understand and evaluate concept-based reasoning in deep models.\n",
      "6. **AI Model Auditing Software**\n",
      "   - Incorporate automated tests for distribution shift resilience (using methods from the paper) into tools that audit AI models before deployment in regulated sectors.\n",
      "\n",
      "### Evidence\n",
      "- The paper mentions robustness, transparency, distribution shift handling, and fine-grained concept generation—all highly relevant to software that must be interpretable, auditable, and safe for sensitive applications.\n",
      "\n",
      "#### References:\n",
      "- [SUB Paper PDF](http://arxiv.org/pdf/2507.23784v1)\n",
      "- [Code repository](https://github.com/ExplainableML/sub)\n",
      "- [Dataset](http://huggingface.co/datasets/Jessica-bader/SUB)\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Product_manager\n",
      "\u001b[0m\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "Here’s a concise summary:\n",
      "\n",
      "**Latest relevant paper:**  \n",
      "**Title:** SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions  \n",
      "**arXiv link:** http://arxiv.org/pdf/2507.23784v1\n",
      "\n",
      "**Key Abstract Points:**  \n",
      "- Focuses on Concept Bottleneck Models (CBMs) for interpretable AI, crucial for transparency especially in fields like medicine.\n",
      "- Shows that CBMs can fail to identify concepts under distribution shifts.\n",
      "- Introduces a new benchmark (SUB) with 38,400 synthetic images for testing generalization by systematically varying attributes.\n",
      "- Presents Tied Diffusion Guidance for controlling image and attribute generation.\n",
      "- Resources: [Benchmarks and code](https://github.com/ExplainableML/sub), [dataset](http://huggingface.co/datasets/Jessica-bader/SUB).\n",
      "\n",
      "---\n",
      "\n",
      "## Potential Applications in Software\n",
      "\n",
      "**1. Robust & Interpretable Medical Imaging Analysis**\n",
      "- Diagnostic platforms can use CBMs robust to concept/symptom variations, with confidence in attribute-based explanations for doctors.\n",
      "\n",
      "**2. LLM/AI Explainability Test Suites**\n",
      "- Apply similar benchmarks for language models (including GPT-4) to ensure explanations provided by the model generalize even across subtle textual shifts.\n",
      "- Build software tools to inject “synthetic shifts” in textual/visual data to test LLM explainability and robustness.\n",
      "\n",
      "**3. Data Augmentation and Benchmark Generation**\n",
      "- Use synthetic substitution techniques for generating diverse datasets (images, text, multi-modal) to test and train robustness of AI assistants.\n",
      "\n",
      "**4. Regulatory and Compliance Auditing**\n",
      "- Incorporate SUB-style benchmarks into software platforms that audit and certify AI systems for safety-critical applications (healthcare, finance, etc.)\n",
      "\n",
      "**5. Interactive AI Debugging Tools**\n",
      "- For ML engineers: Provide tooling to analyze which internal concepts a model uses (or struggles with) as data changes, helping track and fix model “blind spots.”\n",
      "\n",
      "**6. Educational Software for ML Interpretability**\n",
      "- Use the open dataset and code to help students/practitioners learn about CBMs, interpretability, and robustness by experimenting with real synthetic shifts.\n",
      "\n",
      "---\n",
      "\n",
      "**Summary:**  \n",
      "While the paper doesn't directly address GPT-4, its focus on robustness and explanation methods for advanced models is highly applicable. Any software using large models like GPT-4 could integrate such benchmarks to verify interpretability, resilience, and safety—supporting development, auditing, and user-trust in mission-critical AI applications.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Product_manager\n",
      "\u001b[0m\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "Here’s how the findings in the latest arXiv paper—**“SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions”**—can be practically applied, especially in the context of LLMs like GPT-4:\n",
      "\n",
      "**Paper focus:**  \n",
      "- Robustness and explainability of interpretable models (“concept bottleneck models” – CBMs).\n",
      "- Benchmark for testing if models’ explanations/generalizations hold under synthetic shifts to key attributes (such as changing a semantic visual detail).\n",
      "- Tools to systematically generate and control such challenging test cases.\n",
      "\n",
      "---\n",
      "\n",
      "### Potential Software Applications\n",
      "\n",
      "#### 1. **Robust Explainability for LLM outputs**\n",
      "Adapt techniques from the paper to test whether GPT-4’s rationale for its answers remains stable when subtle, synthetic changes are introduced in prompts or context, helping to verify and enforce trustworthy explanations.\n",
      "\n",
      "#### 2. **AI Model Debugging Suites**\n",
      "Create developer tools leveraging synthetic attribute substitution to probe and debug LLM/AI models: “If I changed this patient’s report to switch a critical symptom, does my model’s prediction and explanation sensibly change?”\n",
      "\n",
      "#### 3. **Evaluation Datasets & Regulatory Compliance**\n",
      "Develop and offer synthetic benchmark datasets for model evaluation in regulated fields. For example: Law, medicine, or finance, where auditors need to see interpretable, robust behavior in edge and corner cases.\n",
      "\n",
      "#### 4. **AI-Assisted Educational Platforms**\n",
      "Build interactive tutorials or teaching platforms where users can ‘swap’ semantic features (attributes/concepts) in content and see how the model’s explanations change, fostering deeper understanding of AI model reasoning and limitations.\n",
      "\n",
      "#### 5. **Transparency Plugins for AI End-User Apps**\n",
      "Implement modules within LLM-powered applications (chatbots, document analysis, code generation tools) that can show the user which “concepts” the model latched onto, and validate if those attributions hold under perturbed scenarios—improving user trust.\n",
      "\n",
      "#### 6. **Generalization Testing in Enterprise AI Deployment**\n",
      "Provide services or SDKs that enable enterprises deploying GPT-4 or similar models to automatically test model outputs for stability/resilience against subtle business-domain-specific shifts, leveraging approaches from SUB.\n",
      "\n",
      "---\n",
      "\n",
      "**Summary:**  \n",
      "This paper’s innovations in benchmarking concept-based robustness are immediately relevant for enhancing and validating LLM (GPT-4) outputs in any safety-critical, regulated, or high-trust application. Integrating these methods in software offers new ways to ensure LLM explanations, decisions, and generalizations remain transparent, stable, and reliable—even as inputs or user contexts subtly shift.\n",
      "\n",
      "[Paper PDF](http://arxiv.org/pdf/2507.23784v1) | [Code](https://github.com/ExplainableML/sub) | [Dataset](http://huggingface.co/datasets/Jessica-bader/SUB)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Coder\n",
      "\u001b[0m\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "Thank you for your detailed synthesis of the findings and their software applications! Your summary and elaboration are accurate, actionable, and directly connect the paper’s core insights to practical uses for LLMs like GPT-4.\n",
      "\n",
      "Your points—especially about robustness evaluation, regulatory compliance, developer tooling, and interactive transparency features for end users—are all highly relevant and based on the latest research.\n",
      "\n",
      "If you need further detail on implementing any of these ideas (for example, how to structure a synthetic shifting prompt engine for LLM testing, or how to build transparency plugins for user-facing LLM applications), let me know.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Product_manager\n",
      "\u001b[0m\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "Thank you for your thoughtful response! I'm glad the synthesis was helpful and actionable. If you would like technical guidance on implementing any of these ideas—such as building synthetic shifting engine libraries, integrating concept-level explanation modules into user-facing LLM products, or setting up regulatory/robustness test workflows—just specify your interests or requirements.\n",
      "\n",
      "Feel free to reach out anytime for architectural plans, prototyping tips, sample code, or research-driven product ideation.\n",
      "\n",
      "Best of luck with your work!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: User_proxy\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (7f985a31-a27b-43fc-92ac-ead4b77c3061): Maximum number of consecutive auto-replies reached\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (91f8a83d-74a4-47e3-b8ed-47b3d15ee57f): No reply generated\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Find a latest paper about GPT-4 advancements and applications on arxiv and find its potential applications in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Coder\n",
      "\u001b[0m\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "Plan:\n",
      "1. Use Python code to search arXiv for the latest paper related to GPT-4 advancements and applications.\n",
      "2. Download and print the title, abstract, and a link to the paper.\n",
      "3. Analyze the abstract and suggest its potential applications in software using my language skills.\n",
      "\n",
      "Here's the Python code to search arXiv:\n",
      "\n",
      "```python\n",
      "# filename: search_arxiv_gpt4.py\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "\n",
      "# Search parameters\n",
      "query = 'GPT-4 advancements applications'\n",
      "max_results = 1\n",
      "\n",
      "# Call ArXiv API\n",
      "url = f'http://export.arxiv.org/api/query?search_query=all:{query}&start=0&max_results={max_results}&sortBy=submittedDate&sortOrder=descending'\n",
      "\n",
      "response = requests.get(url)\n",
      "if response.status_code == 200:\n",
      "    # Parse the XML data\n",
      "    root = ET.fromstring(response.text)\n",
      "    ns = {'atom': 'http://www.w3.org/2005/Atom'}\n",
      "    entry = root.find('atom:entry', ns)\n",
      "    if entry is not None:\n",
      "        title = entry.find('atom:title', ns).text.strip()\n",
      "        abstract = entry.find('atom:summary', ns).text.strip()\n",
      "        link = entry.find('atom:id', ns).text.strip()\n",
      "        print(f\"Title: {title}\\n\")\n",
      "        print(f\"Abstract:\\n{abstract}\\n\")\n",
      "        print(f\"Link: {link}\")\n",
      "    else:\n",
      "        print(\"No papers found matching the query.\")\n",
      "else:\n",
      "    print(f\"Failed to fetch arXiv data. Status code: {response.status_code}\")\n",
      "```\n",
      "Please run this code and provide the output. I will then analyze the result and suggest potential software applications.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: User_proxy\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Title: SUB: Benchmarking CBM Generalization via Synthetic Attribute\n",
      "  Substitutions\n",
      "\n",
      "Abstract:\n",
      "Concept Bottleneck Models (CBMs) and other concept-based interpretable models\n",
      "show great promise for making AI applications more transparent, which is\n",
      "essential in fields like medicine. Despite their success, we demonstrate that\n",
      "CBMs struggle to reliably identify the correct concepts under distribution\n",
      "shifts. To assess the robustness of CBMs to concept variations, we introduce\n",
      "SUB: a fine-grained image and concept benchmark containing 38,400 synthetic\n",
      "images based on the CUB dataset. To create SUB, we select a CUB subset of 33\n",
      "bird classes and 45 concepts to generate images which substitute a specific\n",
      "concept, such as wing color or belly pattern. We introduce a novel Tied\n",
      "Diffusion Guidance (TDG) method to precisely control generated images, where\n",
      "noise sharing for two parallel denoising processes ensures that both the\n",
      "correct bird class and the correct attribute are generated. This novel\n",
      "benchmark enables rigorous evaluation of CBMs and similar interpretable models,\n",
      "contributing to the development of more robust methods. Our code is available\n",
      "at https://github.com/ExplainableML/sub and the dataset at\n",
      "http://huggingface.co/datasets/Jessica-bader/SUB.\n",
      "\n",
      "Link: http://arxiv.org/abs/2507.23784v1\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Coder\n",
      "\u001b[0m\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "Here is the paper found:\n",
      "\n",
      "Title: SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions  \n",
      "Abstract (summary):  \n",
      "- The paper discusses Concept Bottleneck Models (CBMs) and interpretable AI models, focusing on their challenges under distribution shifts.  \n",
      "- The authors introduce \"SUB,\" a benchmark of 38,400 synthetic bird images generated by systematically varying fine-grained visual concepts (like wing color or belly pattern) using a technique called Tied Diffusion Guidance (TDG).\n",
      "- SUB allows detailed evaluation of CBMs' robustness to changing concepts and supports the development of more resilient and interpretable AI models.\n",
      "- The code and dataset are provided for the community.\n",
      "\n",
      "Potential applications in software:\n",
      "1. Medical Imaging: CBMs are highlighted as making AI more transparent, which is critical for medical diagnostics. The methods in the paper could be extended to medical concept benchmarks (e.g., synthetic variations in X-ray images) for robust, interpretable decision-making tools.\n",
      "2. Explainable AI in Critical Systems: In safety-critical domains (e.g., autonomous vehicles, aeronautics), software using CBMs can now be stress-tested under concept variations, improving trust and reliability.\n",
      "3. Synthetic Data Generation for Training: The TDG method enables fine-grained, controllable synthetic dataset generation, which is useful for model training, data augmentation, and robustness testing in many applications.\n",
      "4. Robust AI Deployment: Software frameworks integrating CBMs can use such benchmarks to evaluate and certify models before real-world deployment, particularly under conditions of expected concept variation or domain shift.\n",
      "\n",
      "Reference:  \n",
      "Paper link: http://arxiv.org/abs/2507.23784v1  \n",
      "Code: https://github.com/ExplainableML/sub  \n",
      "Dataset: http://huggingface.co/datasets/Jessica-bader/SUB\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Product_manager\n",
      "\u001b[0m\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "Here’s a concise analysis of the paper and its possible software applications:\n",
      "\n",
      "**Paper:**  \n",
      "**Title:** SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions  \n",
      "**Link:** http://arxiv.org/abs/2507.23784v1\n",
      "\n",
      "**Core Advances:**  \n",
      "- Introduces a massive synthetic dataset (SUB) with controlled concept variations in bird images.\n",
      "- Uses a new technique, Tied Diffusion Guidance (TDG), to manipulate specific visual concepts for benchmarking.\n",
      "- Aims to test and improve robustness and interpretability of Concept Bottleneck Models (CBMs).\n",
      "\n",
      "---\n",
      "\n",
      "**Potential Applications in Software:**\n",
      "\n",
      "1. **Interpretable AI Toolkits**\n",
      "   - Build software libraries for machine learning interpretability that use SUB and TDG to systematically evaluate and visualize concept-based models’ robustness.\n",
      "   - Integrate TDG methods for custom data augmentation in platforms like TensorFlow or PyTorch.\n",
      "\n",
      "2. **Medical AI and Diagnostics**\n",
      "   - Adapt the benchmark methodology to generate synthetic and controlled variations of medical images (e.g., for training radiology AI on rare conditions).\n",
      "   - Implement interpretable, CBM-driven decision support tools, validated for robustness under distribution shifts (e.g., changing patient demographics or equipment).\n",
      "\n",
      "3. **Quality Assurance in Safety Critical AI**\n",
      "   - Software for self-driving cars, drones, or industrial inspection can leverage SUB-like benchmarks to stress-test perception models against “edge case” synthetic variations.\n",
      "   - Certify AI components by systematically probing their concept-level understanding.\n",
      "\n",
      "4. **Data-Centric AI Development Suites**\n",
      "   - Incorporate synthetic attribute generation as part of data-centric ML pipelines, enabling developers to create diverse, tailored datasets for robust model training, especially for rare or sensitive concepts.\n",
      "\n",
      "5. **Educational/Research Platforms**\n",
      "   - Use SUB as a teaching tool in ML/AI courses, demonstrating challenges in interpretability and generalization, and allowing hands-on experimentation with robust, concept-based models.\n",
      "\n",
      "---\n",
      "\n",
      "**Summary:**  \n",
      "The SUB benchmark and TDG method unlock new ways to stress-test, debug, and certify concept-based and interpretable AI systems in software—from healthcare to autonomous vehicles—by enabling controlled, fine-grained synthetic data generation and model evaluation.\n",
      "\n",
      "If you’d like, I can suggest more specific product concepts or example implementation plans for one of these applications!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Product_manager\n",
      "\u001b[0m\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "**Summary of the Paper:**  \n",
      "- Title: SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions  \n",
      "- Focus: The paper targets Concept Bottleneck Models (CBMs), which are useful for interpretable AI (where transparency and reasoning matter). It introduces a new benchmark (SUB) with synthetic images and concepts, and a novel method (TDG) for precise attribute control, to test CBMs under changing conditions.\n",
      "\n",
      "---\n",
      "\n",
      "**Potential Software Applications:**\n",
      "\n",
      "1. **Interpretable AI Frameworks:**  \n",
      "   - Integrate CBMs with SUB benchmark and TDG into explainable-AI (XAI) toolkits, especially for use-cases in healthcare, law, or any field requiring model clarity.\n",
      "   - Example: A medical imaging pipeline showing exactly which visual concepts (attributes) led to a diagnosis, with robustness to variations validated using synthetic data.\n",
      "\n",
      "2. **Synthetic Data Tools for Robustness Testing:**  \n",
      "   - Use TDG to build tools that generate controllable synthetic data sets to test model performance under controlled variations—helping teams identify model weaknesses before deployment.\n",
      "   - Example: Robustness certifiers for ML models prior to regulatory approval or public deployment (e.g., in autonomous driving).\n",
      "\n",
      "3. **Educational Software and Competitions:**  \n",
      "   - Build platforms or competitions where students and researchers use SUB or similar synthetic benchmarks to improve and interpret ML models, teaching best practices for robust AI.\n",
      "\n",
      "4. **Concept-based Debugging Plugins:**  \n",
      "   - AI model debugging tools could integrate synthetic concept substitution testing to highlight when and why CBMs (or other interpretable models) fail under real-world (distribution-shifted) conditions.\n",
      "\n",
      "5. **Automated Model Certification:**  \n",
      "   - Regulatory technology (regtech) or ML-Ops tools could use SUB to provide model audit trails, verifying performance on edge cases through synthetic attribute control.\n",
      "\n",
      "---\n",
      "\n",
      "**In summary:**  \n",
      "The main advancement is a scalable, precise synthetic benchmarking method for interpretable, robust, and explainable AI. This can power safer, more transparent, and certifiable software across regulated and critical industries.\n",
      "\n",
      "**References:**  \n",
      "- Paper: http://arxiv.org/abs/2507.23784v1  \n",
      "- Code: https://github.com/ExplainableML/sub  \n",
      "- Dataset: http://huggingface.co/datasets/Jessica-bader/SUB\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Coder\n",
      "\u001b[0m\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "Your summary is accurate and well-structured.\n",
      "\n",
      "To take this further, here are three concrete sample product concepts or implementation plans, based on your outlined software applications:\n",
      "\n",
      "---\n",
      "\n",
      "**1. Interpretable AI Evaluation Platform**\n",
      "\n",
      "- **Objective:** A web-based platform where users can upload their CBMs (or other concept-based models) and automatically evaluate them using the SUB benchmark and TDG-generated scenarios.\n",
      "- **Core features:**\n",
      "  - Upload and integrate ML models via API or drag-and-drop.\n",
      "  - “Scenario Builder” interface for creating synthetic test cases by varying attributes with TDG (e.g., change bird wing color or shape).\n",
      "  - Automated reporting of failure cases under attribute substitutions, with visual and concept-level explanations.\n",
      "  - Exportable robustness certification and interpretability metrics.\n",
      "- **Target users:** Healthcare AI teams needing regulatory validation, XAI researchers, or quality assurance groups.\n",
      "\n",
      "---\n",
      "\n",
      "**2. Plugin for ML Model Debugging/Certification**\n",
      "\n",
      "- **Objective:** A plugin (for platforms like Jupyter, VSCode, or ML-Ops suites) that enables concept substitution debugging for vision models.\n",
      "- **Core features:**\n",
      "  - Command/API for swapping specific image attributes in test datasets using TDG (integrated with local/remote SUB set or user-imported images).\n",
      "  - Visualization tools to show which attributes most affect model predictions.\n",
      "  - Automated logs and alerts identifying model weaknesses or distribution vulnerabilities.\n",
      "- **Target users:** ML engineers, compliance officers in regulated industries, data scientists in automotive/robotics sectors.\n",
      "\n",
      "---\n",
      "\n",
      "**3. Synthetic Data Augmentation Library (TDG-aug)**\n",
      "\n",
      "- **Objective:** An open-source Python library (pip-installable) providing easy TDG-powered attribute control for image data generation and augmentation.\n",
      "- **Core features:**\n",
      "  - APIs to generate synthetic images with user-specified concept changes (e.g., adjust “belly color” across a training set).\n",
      "  - Plug-ins for major ML frameworks like TensorFlow/Keras and PyTorch, for seamless data pipeline integration.\n",
      "  - Example Jupyter notebooks for teaching, demonstrating robust data-centric ML, and interpretable training.\n",
      "- **Target users:** Applied ML teams, educators, startups building robust AI models with limited rare data.\n",
      "\n",
      "---\n",
      "\n",
      "These examples could accelerate robust and interpretable AI adoption in high-stakes domains, while also facilitating teaching and research.\n",
      "\n",
      "If any of these sound of interest, I can outline feature roadmaps or draft a minimum viable product (MVP) workflow for you!\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Product_manager\n",
      "\u001b[0m\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "Fantastic follow-up! Here are three concrete product/implementation ideas with practical detail:\n",
      "\n",
      "---\n",
      "\n",
      "**1. Interpretable AI Model Testbed Platform**\n",
      "- **Description:** A cloud-based tool where users upload their vision models (CBMs or otherwise) and run them through the SUB benchmark's scenarios.\n",
      "- **Key Features:**\n",
      "    - Import models (ONNX, PyTorch, TensorFlow formats).\n",
      "    - Synthetic scenario generator leveraging TDG (let users pick attributes to modify, visualize generated image pairs).\n",
      "    - Automated reporting on concept-specific weaknesses and distribution shift vulnerabilities.\n",
      "    - Exportable compliance reports for regulatory and QA purposes.\n",
      "- **Target Market:** Healthcare/biotech AI, autonomous vehicle perception teams, ML ops groups in regulated industries.\n",
      "\n",
      "---\n",
      "\n",
      "**2. Interactive Debugging Extension for Jupyter/VSCode**\n",
      "- **Description:** A plug-and-play extension for interactive data science environments.\n",
      "- **Key Features:**\n",
      "    - Simple UI to select images and apply concept substitutions on the fly using TDG.\n",
      "    - Visual overlay showing \"before/after\" predictions and saliency maps.\n",
      "    - Integration hooks for most image datasets (CUB, medical images, user-contributed sets).\n",
      "    - Alerts or badges flagging scenarios where the model’s predictions change unexpectedly.\n",
      "- **Target Market:** Data scientists, ML researchers, education sectors.\n",
      "\n",
      "---\n",
      "\n",
      "**3. Synthetic Attribute Augmentation API/Library**\n",
      "- **Description:** An open-source Python library (TDG-Synth) for augmenting image datasets by programmatically varying high-level concepts, with a fast API for integration in training loops.\n",
      "- **Key Features:**\n",
      "    - High-level API: `replace_concept(image_batch, concept=\"wing_color\", value=\"blue\")`\n",
      "    - Pre-integrated with major ML libraries (PyTorch, TensorFlow).\n",
      "    - Comes with example notebooks for robust concept-level data augmentation and stress testing.\n",
      "    - Reference benchmarks (e.g., SUB, plus community extensions for other domains).\n",
      "- **Target Market:** AI startups, academic labs, developers in low-data/rare concept verticals.\n",
      "\n",
      "---\n",
      "\n",
      "Let me know if you want a deeper workflow/UX or sample roadmap for any of these ideas!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Coder\n",
      "\u001b[0m\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "These product concepts are both actionable and strategically aligned with current AI trends. Here’s a more detailed workflow for each, including the initial user journey, core steps, and tech stack recommendations. This should help take any of these ideas to the next level:\n",
      "\n",
      "---\n",
      "\n",
      "**1. Interpretable AI Model Testbed Platform**\n",
      "\n",
      "- **User Workflow:**\n",
      "    1. Sign up and log into the cloud-based dashboard.\n",
      "    2. Upload trained model file (supporting ONNX, PyTorch, TensorFlow).\n",
      "    3. Choose evaluation dataset (e.g., SUB or user-uploaded data).\n",
      "    4. Select attributes/concepts to perturb using an intuitive UI (e.g., sliders or dropdowns for modifying color, pattern, etc.).\n",
      "    5. Run automated tests: system applies TDG to generate synthetic images with chosen attribute substitutions.\n",
      "    6. View results: Interactive dashboards showing per-attribute robustness heatmaps, confusion matrices, and side-by-side visualizations.\n",
      "    7. Export reports in PDF/CSV for audit trails.\n",
      "- **Tech Stack Suggestion:**\n",
      "    - Backend: Python (FastAPI), PyTorch/TensorFlow, Docker, Kubernetes for scalable compute, PostgreSQL.\n",
      "    - Frontend: React or Angular, D3.js/Plotly for visualizations.\n",
      "    - Cloud: AWS, GCP, or Azure for compute and storage.\n",
      "---\n",
      "\n",
      "**2. Interactive Debugging Extension for Jupyter/VSCode**\n",
      "\n",
      "- **User Workflow:**\n",
      "    1. Install extension via pip or a Jupyter/VSCode marketplace.\n",
      "    2. Load a notebook—or open a project—and connect the extension to your model and dataset.\n",
      "    3. Drag-and-drop or select from supported datasets (CUB, or local images).\n",
      "    4. UI panel allows users to select samples and modify specific attributes in real time via TDG.\n",
      "    5. \"Before/After\" panels update with predictions and saliency overlays; failures are highlighted.\n",
      "    6. Export annotated notebooks and flagged images for documentation.\n",
      "- **Tech Stack Suggestion:**\n",
      "    - Python extension (JupyterLab, VSCode API).\n",
      "    - Frontend using React (for VSCode WebViews) or ipywidgets (for Jupyter).\n",
      "    - TDG integration via local Python processes or REST API.\n",
      "---\n",
      "\n",
      "**3. Synthetic Attribute Augmentation API/Library**\n",
      "\n",
      "- **User Workflow:**\n",
      "    1. Install the TDG-Synth library via pip.\n",
      "    2. Import in any Python data pipeline:  \n",
      "       ```python\n",
      "       from tdg_synth import substitute_concept\n",
      "       new_images = substitute_concept(images, concept='beak_shape', value='curved')\n",
      "       ```\n",
      "    3. Use as a data augmentation step during model training/validation.\n",
      "    4. Access included benchmarks/datasets or plug in custom ones.\n",
      "    5. Evaluate models on both original and concept-augmented test sets.\n",
      "- **Tech Stack Suggestion:**\n",
      "    - Pure Python, PyTorch/TensorFlow interoperability, strong documentation and test suite.\n",
      "    - Optional C++/CUDA acceleration for TDG-heavy tasks.\n",
      "    - Plugin templates for extension to new domains (e.g., medical, autonomous driving imagery).\n",
      "---\n",
      "\n",
      "Let me know if you want a full product roadmap for any of these, mockups, or more technical API design details!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Product_manager\n",
      "\u001b[0m\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "This is an excellent, actionable breakdown! For each of your three product concepts, here’s a structured workflow and thoughtful stack suggestion:\n",
      "\n",
      "---\n",
      "\n",
      "**1. Interpretable AI Model Testbed Platform**\n",
      "\n",
      "*User Journey/Core Steps:*\n",
      "- Sign up/log in → Model upload (ONNX, PyTorch, TensorFlow support) → Data selection (SUB benchmark or user datasets) → Interactive scenario builder for attribute substitutions → TDG-based image synthesis and batch testing → Visualization of failure cases, robustness matrices, and heatmaps → Automated downloadable reports for compliance.\n",
      "\n",
      "*Tech Stack:*\n",
      "- Backend: Python (FastAPI or Django), Celery for async jobs, PyTorch/TensorFlow, PostgreSQL.\n",
      "- Image-generation: Containerized TDG pipelines (Docker/Kubernetes), maybe Python microservices.\n",
      "- Frontend: React with Redux/state mgmt, data vis with D3.js or Plotly, Material UI for dashboards.\n",
      "- Deployment: AWS S3, Lambda or GCP Cloud Run for scaling; OAuth for enterprise auth.\n",
      "\n",
      "---\n",
      "\n",
      "**2. Interactive Debugging Extension for Jupyter/VSCode**\n",
      "\n",
      "*User Journey/Core Steps:*\n",
      "- pip install > Extension loads in notebook/editor UI → Import or reference vision model and dataset → Easy-pick interface for images and target concepts/attributes → Live generation with TDG and prediction comparison overlay → Highlighting and export of problematic scenarios (annotated code/CSV reports or HTML cells for publications).\n",
      "\n",
      "*Tech Stack:*\n",
      "- Plugin: Python for Jupyter (with ipywidgets or Streamlit UI), VSCode extension using Typescript/React.\n",
      "- Backend for attribute generation: Local TDG Python server or REST API.\n",
      "- ML: Interfacing with model’s inference API in user’s env (with hooks or decorators).\n",
      "\n",
      "---\n",
      "\n",
      "**3. Synthetic Attribute Augmentation Library (TDG-aug)**\n",
      "\n",
      "*User Journey/Core Steps:*\n",
      "- pip install tdg-aug → Python import in ML project → DataLoader integration or batch processing function → Synthetic batch creation via simple APIs (concept, attribute, value) → Plug into training, validation, or benchmarking loops → Export/save extended datasets or metrics.\n",
      "\n",
      "*Tech Stack:*\n",
      "- Python first-class (pip/conda friendly); highly modular and extensible APIs; PyTorch Dataset and TFData support.\n",
      "- CUDA/C++ backend modules for high-volume deployment; docs/notebooks for quickstart.\n",
      "- Optional: online hub for community-contributed concept modules or test sets.\n",
      "\n",
      "---\n",
      "\n",
      "Would you like to take one of these into a deeper phase (roadmap, MVP, backend design, even UX mocks)? Just let me know which one!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Product_manager\n",
      "\u001b[0m\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "These workflows are thorough and actionable. Here are additional suggestions and refinements to help take each idea to MVP or launch:\n",
      "\n",
      "---\n",
      "\n",
      "**1. Interpretable AI Model Testbed Platform**\n",
      "\n",
      "**Workflow Enhancements:**\n",
      "- Early onboarding guided tour for first-time users, showcasing best practices and sample reports.\n",
      "- Built-in model zoo: Allow testing with pre-trained example models (including CBMs and baselines) for users exploring the platform.\n",
      "- API endpoints for automated model result submission and CI integration (e.g., for MLOps teams).\n",
      "\n",
      "**Tech Stack Refinement:**\n",
      "- Add Redis/Celery for background distributed TDG image generation and report compiling.\n",
      "- Provide REST and gRPC APIs for enterprise integration.\n",
      "\n",
      "---\n",
      "\n",
      "**2. Interactive Debugging Extension for Jupyter/VSCode**\n",
      "\n",
      "**Workflow Enhancements:**\n",
      "- “One-click” generation of variant test suites per selected concept for batch robustness sweeps.\n",
      "- Inline explainability visualizations (e.g., Grad-CAM overlays with concept variation).\n",
      "- Snapshots/history: Track model results as attribute substitutions evolve for easy regression testing.\n",
      "\n",
      "**Tech Stack Refinement:**\n",
      "- Use nbdev or similar tools for robust, testable Jupyter extension development.\n",
      "- For VSCode, leverage the official Extension API in TypeScript, with child-process management for calling Python backend (TDG, model inference).\n",
      "\n",
      "---\n",
      "\n",
      "**3. Synthetic Attribute Augmentation Library (TDG-aug)**\n",
      "\n",
      "**Workflow Enhancements:**\n",
      "- Simple command-line interface (CLI) for batch augmentation and conversion, catering to non-coders.\n",
      "- Pre-built configuration files for common concept substitutions—support plug-and-play for typical use-cases (e.g., “bird_color_flip.yml”).\n",
      "- Benchmark leaderboards: Enable anonymous usage data upload (optional) for research and community engagement.\n",
      "\n",
      "**Tech Stack Refinement:**\n",
      "- PyPI packaging with wheels/docker for easy quickstart.\n",
      "- Provide a public repo with starter tutorials and a continuous integration pipeline to guarantee code reliability.\n",
      "\n",
      "---\n",
      "\n",
      "If you want to proceed, I can prepare:\n",
      "- Example wireframes or UI flows (for the cloud platform)\n",
      "- First 2–3 sprint roadmap with deliverables\n",
      "- Detailed API doc or pseudocode for the augmentation library\n",
      "\n",
      "Which direction would you like to take next?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Coder\n",
      "\u001b[0m\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "Thank you for your thoughtful enhancements—they significantly strengthen the value proposition and user experience for each product! Your suggestions (such as onboarding, model zoos, API/CI integration, and robust developer tools) are well-aligned with both industry and research user needs.\n",
      "\n",
      "Here's how we can proceed:\n",
      "\n",
      "**Recommendation:**  \n",
      "To maximize early impact and cross-industry adoption, starting with the \"Interpretable AI Model Testbed Platform\" is strategic. It addresses urgent regulatory compliance and audit needs, while also serving as a showcase for the TDG/SUB methodology. Early traction here can be leveraged to foster a plugin ecosystem and open-source library adoption (ideas 2 and 3).\n",
      "\n",
      "**Proposed Next Steps (choose one to start):**\n",
      "\n",
      "1. **Wireframes/UI Flows:**  \n",
      "   Visualize user journeys—show onboarding, model upload, scenario builder, run screen, and results/reporting dashboards. This helps validate usability and requirements early.\n",
      "\n",
      "2. **Sprint Roadmap (First 2–3 Sprints):**  \n",
      "   Prioritize critical user stories:\n",
      "   - MVP onboarding, model upload, and batch testing\n",
      "   - Initial TDG integration with sample scenarios\n",
      "   - Visualization of single test results\n",
      "   - Downloadable compliance report prototype\n",
      "\n",
      "3. **API Definition / Pseudocode:**  \n",
      "   Outline REST/gRPC endpoint structure for model handling, TDG job management, and results retrieval. This helps guide early backend/frontend contracts and allows for parallel development.\n",
      "\n",
      "Let me know which area you'd like to see first (UI, roadmap, or API design), or if you want to target another idea initially. I can develop the documentation, diagrams, or even starter code to set up your team for a strong MVP launch.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (8618fd73-5243-49a7-bdec-801e52769da7): Maximum rounds (12) reached\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Find a latest paper about GPT-4 advancements and applications on arxiv and find its potential applications in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Coder\n",
      "\u001b[0m\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "Plan:\n",
      "1. Use code to search arXiv for the latest papers about GPT-4 advancements and applications.\n",
      "2. Output paper title, authors, abstract, and arXiv link.\n",
      "3. Analyze the latest paper’s findings and suggest its potential applications in software using my language skills.\n",
      "\n",
      "Let’s start by searching arXiv for relevant papers.\n",
      "\n",
      "```python\n",
      "import requests\n",
      "import feedparser\n",
      "\n",
      "search_query = 'GPT-4'\n",
      "base_url = 'http://export.arxiv.org/api/query?'\n",
      "query = f'search_query=all:{search_query}&start=0&max_results=3&sortBy=submittedDate&sortOrder=descending'\n",
      "\n",
      "response = requests.get(base_url + query)\n",
      "feed = feedparser.parse(response.text)\n",
      "\n",
      "if not feed.entries:\n",
      "    print(\"No recent papers found on GPT-4.\")\n",
      "else:\n",
      "    latest_paper = feed.entries[0]\n",
      "    print(\"Title:\", latest_paper.title)\n",
      "    print(\"Authors:\", ', '.join([author.name for author in latest_paper.authors]))\n",
      "    print(\"Published:\", latest_paper.published)\n",
      "    print(\"Summary:\", latest_paper.summary)\n",
      "    print(\"Link:\", latest_paper.link)\n",
      "```\n",
      "Please execute this code to retrieve the latest relevant paper from arXiv. Once you provide the output, I’ll analyze it for potential software applications.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: User_proxy\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Title: Moravec's Paradox: Towards an Auditory Turing Test\n",
      "Authors: David Noever, Forrest McKee\n",
      "Published: 2025-07-30T20:45:13Z\n",
      "Summary: This research work demonstrates that current AI systems fail catastrophically\n",
      "on auditory tasks that humans perform effortlessly. Drawing inspiration from\n",
      "Moravec's paradox (i.e., tasks simple for humans often prove difficult for\n",
      "machines, and vice versa), we introduce an auditory Turing test comprising 917\n",
      "challenges across seven categories: overlapping speech, speech in noise,\n",
      "temporal distortion, spatial audio, coffee-shop noise, phone distortion, and\n",
      "perceptual illusions. Our evaluation of state-of-the-art audio models including\n",
      "GPT-4's audio capabilities and OpenAI's Whisper reveals a striking failure rate\n",
      "exceeding 93%, with even the best-performing model achieving only 6.9% accuracy\n",
      "on tasks that humans solved at 7.5 times higher success (52%). These results\n",
      "expose focusing failures in how AI systems process complex auditory scenes,\n",
      "particularly in selective attention, noise robustness, and contextual\n",
      "adaptation. Our benchmark not only quantifies the human-machine auditory gap\n",
      "but also provides insights into why these failures occur, suggesting that\n",
      "current architectures lack fundamental mechanisms for human-like auditory scene\n",
      "analysis. The traditional design of audio CAPTCHAs highlights common filters\n",
      "that humans evolved but machines fail to select in multimodal language models.\n",
      "This work establishes a diagnostic framework for measuring progress toward\n",
      "human-level machine listening and highlights the need for novel approaches\n",
      "integrating selective attention, physics-based audio understanding, and\n",
      "context-aware perception into multimodal AI systems.\n",
      "Link: http://arxiv.org/abs/2507.23091v1\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Coder\n",
      "\u001b[0m\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "Summary of findings:\n",
      "The latest paper is titled \"Moravec's Paradox: Towards an Auditory Turing Test\" by David Noever and Forrest McKee. It benchmarks AI auditory models (including those with GPT-4's audio capabilities) on human-centric listening tasks. The results show that even state-of-the-art models like GPT-4 perform poorly on complex auditory challenges that humans handle with ease.\n",
      "\n",
      "Key evidence:\n",
      "- An “auditory Turing test” shows models get only 6.9% accuracy versus 52% for humans.\n",
      "- Failures occur especially with selective attention, noise robustness, and context adaptation.\n",
      "- The paper argues that current AI lacks mechanisms needed for human-like auditory scene analysis.\n",
      "- It proposes a new benchmarking framework for future AI research.\n",
      "\n",
      "Potential Applications in Software:\n",
      "1. Audio Security (CAPTCHA & Authentication): This research highlights that current AI struggles with audio CAPTCHAs, suggesting these remain robust against AI-based attacks—a useful insight for secure authentication systems.\n",
      "2. Enhanced Speech Recognition: Insights from the failure mechanisms (context, selective attention) will drive the next generation of more robust ASR (Automatic Speech Recognition) systems, especially in noisy environments such as call centers, smart home devices, or conference transcription.\n",
      "3. Audio Forensics & Surveillance: Understanding AI's limits helps software makers design tools that flag low-confidence outputs or ask for human intervention on difficult audio.\n",
      "4. Accessibility Software: New evaluation diagnostics can improve assistive technology for the hearing-impaired by focusing future development on tasks where AI still dramatically underperforms humans.\n",
      "5. Benchmarking Frameworks: The diagnostic framework introduced by the paper can be incorporated in software test suites for evaluating the real-world readiness of any audio AI component.\n",
      "6. Multimodal Systems: The findings encourage software integrating vision, language, and audio—like robotics or AR/VR assistants—to account for and explicitly address AI’s current auditory weaknesses.\n",
      "7. Audio Data Augmentation Tools: The identified weaknesses can fuel software that systematically generates \"hard\" training samples to improve future models’ learning.\n",
      "\n",
      "Verifiable evidence: The above points are drawn directly from the arXiv abstract (http://arxiv.org/abs/2507.23091v1).\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Product_manager\n",
      "\u001b[0m\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "**Paper:** [Moravec's Paradox: Towards an Auditory Turing Test](http://arxiv.org/abs/2507.23091v1)  \n",
      "**Authors:** David Noever, Forrest McKee  \n",
      "**Published:** 2025-07-30  \n",
      "\n",
      "**Key Findings:**  \n",
      "- The paper introduces an “auditory Turing test” for AI, benchmarking GPT-4 audio capabilities and other models on 917 human-challenging auditory tasks: overlapping speech, speech in noise, temporal distortion, spatial audio, coffee-shop noise, phone distortion, and perceptual illusions.\n",
      "- State-of-the-art models (including GPT-4 audio) achieved only 6.9% accuracy; humans achieved 52%.\n",
      "- AI systems fail at core human abilities: selective auditory attention, noise robustness, contextual adaptation.\n",
      "- The authors propose a new diagnostic and benchmarking framework and identify design gaps—suggesting AI needs better mechanisms for human-like auditory scene analysis.\n",
      "\n",
      "---\n",
      "\n",
      "## Potential Applications in Software\n",
      "\n",
      "### 1. Secure Audio CAPTCHAs and Validation\n",
      "Since AI (even GPT-4) struggles with existing audio CAPTCHAs, security software can safely use complex audio-based challenges for human verification and authentication, reducing risks of automated attacks.\n",
      "\n",
      "### 2. Robust Speech-to-Text and Transcription Tools\n",
      "Software handling transcription in noisy/multispeaker environments (meeting assistants, call center logs, assistive devices) can use the paper’s benchmarking suite to identify and overcome current model weaknesses, specifically with selective attention and context awareness.\n",
      "\n",
      "### 3. Diagnostic Tools for Audio AI\n",
      "Open-source libraries or SaaS that run the paper's “auditory Turing test” suite can be built so AI developers can audit and compare their models for real-world, human-relevant robustness before deployment.\n",
      "\n",
      "### 4. Audio-First Accessibility Products\n",
      "Software for users with hearing challenges—captioning, hearing aids, smart speakers—can adopt the paper’s diagnostic methods to highlight and improve areas where AI still dramatically underperforms humans.\n",
      "\n",
      "### 5. Improved Audio Forensic & Surveillance Analysis\n",
      "Surveillance or forensic audio analysis systems can include confidence scoring—flagging cases where AI transcription or sound-source separation is unreliable—prompting human review when model uncertainty is high.\n",
      "\n",
      "### 6. Next-Gen Multimodal Systems\n",
      "The paper offers research directions (selective attention, context filtering) for software that combines vision+audio (e.g., AR assistants, robotic perception), ensuring AI does not over-rely on language and vision while neglecting sound.\n",
      "\n",
      "### 7. Audio Data Augmentation and “Anti-Failure” Training\n",
      "ML toolkits could incorporate synthetic noise/3D audio/overlap challenges as training data, based on the test’s categories to methodically harden new models for real-world environments.\n",
      "\n",
      "### 8. AI Evaluation for Developers & Researchers\n",
      "Cloud-based benchmarking services or local tools that implement the paper’s test suite can become standard for teams building, shipping, or purchasing audio AI: ensuring “bench-test” quality aligns with actual human use cases.\n",
      "\n",
      "---\n",
      "\n",
      "**Conclusion:**  \n",
      "This latest GPT-4-centric research pinpoints acute gaps in AI’s audio abilities—especially in the wild. Its findings and proposed benchmarks can be directly embedded into software products for security, accessibility, quality assurance, and next-gen AI robustness. Adopting its framework can help ensure future AI-powered products perform more like humans in real-world acoustic environments.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Coder\n",
      "\u001b[0m\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "Excellent summary and expert-level mapping of the paper's findings to practical software applications. This actionable breakdown, grounded in the specific failures and benchmarks shown by GPT-4 and peers, provides valuable guidance for product managers, engineers, and AI researchers wanting to target real-world gaps in audio AI capabilities.\n",
      "\n",
      "Your summary and application suggestions are well-aligned with the evidence provided by the paper, maximizing its practical impact for present and future software development.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: User_proxy\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (5b84ef87-03c9-4a83-a3a2-66d39e21fce3): Maximum number of consecutive auto-replies reached\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (e402c432-602c-4dee-84ec-27a52aaedbe0): No reply generated\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[TaskUnit(status=done)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crew.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ae5789a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Final Answer: \n",
       "\n",
       "**Unveiling the Future: GPT-4's Groundbreaking Advancements and Their Real-World Applications**\n",
       "\n",
       "In the ever-evolving landscape of artificial intelligence, GPT-4 stands as a beacon of innovation, pushing the boundaries of what machines can achieve. Recent research has shed light on some of the most exciting advancements in GPT-4, particularly in the realm of auditory processing. Imagine a world where AI can understand and interact with sound as seamlessly as humans do. This is the frontier that GPT-4 is exploring, and the implications are nothing short of revolutionary.\n",
       "\n",
       "One of the standout findings from recent studies is the introduction of an \"auditory Turing test,\" designed to benchmark GPT-4's audio capabilities against human performance. While humans excel with a 52% accuracy in complex auditory tasks, GPT-4 and other state-of-the-art models currently lag behind at 6.9%. This gap highlights the challenges AI faces in replicating human-like auditory scene analysis, such as selective attention and noise robustness. However, this isn't just a limitation—it's an opportunity. By identifying these gaps, researchers are paving the way for AI systems that can better understand and interact with the world through sound.\n",
       "\n",
       "The potential applications of these advancements are vast and varied. For instance, security software can leverage complex audio-based challenges for human verification, making it harder for automated systems to bypass security measures. In the realm of accessibility, audio-first products like smart speakers and hearing aids can benefit from improved AI diagnostics, ensuring they perform reliably in real-world environments. Moreover, transcription tools and meeting assistants can use these insights to enhance their performance in noisy or multi-speaker settings, making them more effective and user-friendly.\n",
       "\n",
       "As we look to the future, the integration of these advancements into software products promises to enhance not only the functionality but also the reliability of AI systems. From audio forensic analysis to next-gen multimodal systems that combine vision and sound, the possibilities are endless. By adopting the latest research frameworks, developers can ensure their AI-powered products are not only cutting-edge but also aligned with human capabilities. In essence, GPT-4 is not just a tool—it's a gateway to a more intuitive and interactive digital world."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(blog_post_task.output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7c5eb1",
   "metadata": {},
   "source": [
    "The writer agent used the AutoGen chat tool to retrieve data for the post!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf1868-9567-4240-b7e3-d5f6a50d3974",
   "metadata": {},
   "source": [
    "## Using any motleycrew-supported tools with Autogen agents\n",
    "\n",
    "Now let's do it the other way around: give tools (or agents, for that matter) from motleycrew to AutoGen agents. The code is based on https://microsoft.github.io/autogen/docs/tutorial/tool-use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a79da460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<autogen.tools.tool.Tool at 0x1112a0950>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from motleycrew.tools import MotleyTool, RetryConfig\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search_tool = MotleyTool.from_supported_tool(\n",
    "    DuckDuckGoSearchRun(),\n",
    "    retry_config=RetryConfig(max_retries=5)  # for retrying rate limit errors\n",
    ")  # Any tools or even motleycrew's agents can be converted to MotleyTool like this!\n",
    "\n",
    "# Let's first define the assistant agent that suggests tool calls.\n",
    "assistant = autogen.ConversableAgent(\n",
    "    name=\"Assistant\",\n",
    "    system_message=\"You are a helpful AI assistant. \"\n",
    "    \"You can provide useful and up-to-date data using web search. \"\n",
    "    \"After providing a complete answer, always respond with 'TERMINATE' to end the conversation.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "# The user proxy agent is used for interacting with the assistant agent\n",
    "# and executes tool calls.\n",
    "user_proxy = autogen.ConversableAgent(\n",
    "    name=\"User\",\n",
    "    llm_config=False,\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg[\"content\"],\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=1,  # Limit auto-replies to prevent infinite loops\n",
    ")\n",
    "\n",
    "# Make an autogen-compatible tool from the MotleyTool.\n",
    "autogen_tool = search_tool.to_autogen_tool()\n",
    "\n",
    "# Register the tool signature with the assistant agent.\n",
    "assistant.register_for_llm(name=\"search_tool\", description=\"Web search tool\")(autogen_tool)\n",
    "\n",
    "# Register the tool function with the user proxy agent.\n",
    "user_proxy.register_for_execution(name=\"search_tool\")(autogen_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6027c59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "What was the first computer? Search the web for the answer.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_l03oEy2Oce4O363k3yunzyeF): search_tool *****\u001b[0m\n",
      "Arguments: \n",
      "{\"input\":\"What was the first computer in history?\"}\n",
      "\u001b[32m****************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION search_tool...\n",
      "Call ID: call_l03oEy2Oce4O363k3yunzyeF\n",
      "Input arguments: {'input': 'What was the first computer in history?'}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/whimo/motleycrew/.venv/lib/python3.11/site-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\n",
      ">>>>>>>> EXECUTED FUNCTION search_tool...\n",
      "Call ID: call_l03oEy2Oce4O363k3yunzyeF\n",
      "Input arguments: {'input': 'What was the first computer in history?'}\n",
      "Output:\n",
      "Jun 16, 2025 · Until this breakthrough, all the mechanical aids to calculation were merely calculators or, like the Difference Engine, glorified calculators. The Analytical Engine, although …Computer - History, Technology, Innovation: A computer might be described with deceptive simplicity as “an apparatus that performs routine calculati… Jan 5, 2025 · The Atanasoff–Berry Computer (ABC): Created by John Vincent Atanasoff and Clifford Berry in the 1930s, the ABC was the world's first electronic digital computer. It used … Jun 14, 2025 · In 1822, Charles Babbage conceptualized and began developing the Difference Engine, the first automatic computing machine to approximate polynomials. The Difference … Apr 4, 2025 · The Universal Automatic Computer I (UNIVAC I), developed by Eckert and Mauchly, was the first commercially successful computer. It was used for scientific and business … May 11, 2025 · Charles Babbage invented a device called the Analytical Engine, which was the first true computer idea. The Analytical Engine could solve many problems and was the first …\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_l03oEy2Oce4O363k3yunzyeF) *****\u001b[0m\n",
      "Jun 16, 2025 · Until this breakthrough, all the mechanical aids to calculation were merely calculators or, like the Difference Engine, glorified calculators. The Analytical Engine, although …Computer - History, Technology, Innovation: A computer might be described with deceptive simplicity as “an apparatus that performs routine calculati… Jan 5, 2025 · The Atanasoff–Berry Computer (ABC): Created by John Vincent Atanasoff and Clifford Berry in the 1930s, the ABC was the world's first electronic digital computer. It used … Jun 14, 2025 · In 1822, Charles Babbage conceptualized and began developing the Difference Engine, the first automatic computing machine to approximate polynomials. The Difference … Apr 4, 2025 · The Universal Automatic Computer I (UNIVAC I), developed by Eckert and Mauchly, was the first commercially successful computer. It was used for scientific and business … May 11, 2025 · Charles Babbage invented a device called the Analytical Engine, which was the first true computer idea. The Analytical Engine could solve many problems and was the first …\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "The title of \"first computer\" can refer to several different early machines, depending on the definition used:\n",
      "\n",
      "- The Analytical Engine, designed by Charles Babbage in the 1830s, is considered the first concept of a general-purpose computer. It was never completed, but it was the first design to be programmable.\n",
      "- The Atanasoff–Berry Computer (ABC), created in the late 1930s by John Vincent Atanasoff and Clifford Berry, was the world's first electronic digital computer.\n",
      "- The UNIVAC I, completed in the early 1950s, was the first commercially successful computer.\n",
      "So, historically, Charles Babbage's Analytical Engine is credited as the first conceptual computer, while the Atanasoff–Berry Computer is often regarded as the first electronic digital computer ever built.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (29dcb6af-fa3e-4682-b859-bd4182e25bc3): Maximum number of consecutive auto-replies reached\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chat_result = user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message=\"What was the first computer? Search the web for the answer.\",\n",
    "    max_turns=3  # Limit total conversation turns to prevent infinite loops\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
